{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_CW.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhPecMaYD1ft"
      },
      "source": [
        "!pip install wget\r\n",
        "!wget -c https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoScBHK1yn7F"
      },
      "source": [
        "def load_data(df):\r\n",
        "    question1 = df['question1'].astype(str).values\r\n",
        "    question2 = df['question2'].astype(str).values\r\n",
        "    # combined: to get the tokens\r\n",
        "    df['combined'] = df['question1'] + df['question2']\r\n",
        "    labels = df['is_duplicate'].values\r\n",
        "    return question1, question2, labels"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3plKUKHZwr-v"
      },
      "source": [
        "import os\r\n",
        "import re\r\n",
        "import csv\r\n",
        "import codecs\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import SnowballStemmer\r\n",
        "from string import punctuation\r\n",
        "\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\r\n",
        "from keras.layers.merge import concatenate\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
        "import sys"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwIeL0Zyws-y"
      },
      "source": [
        "BASE_DIR = './'\r\n",
        "EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin.gz'\r\n",
        "TRAIN_DATA_FILE = 'drive/MyDrive/NN_CW/quora-question-pairs/train.csv'\r\n",
        "MAX_SEQUENCE_LENGTH = 30\r\n",
        "MAX_NB_WORDS = 200000\r\n",
        "EMBEDDING_DIM = 300\r\n",
        "VALIDATION_SPLIT = 0.1\r\n",
        "\r\n",
        "num_lstm = np.random.randint(175, 275)\r\n",
        "num_dense = np.random.randint(100, 150)\r\n",
        "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\r\n",
        "rate_drop_dense = 0.15 + np.random.rand() * 0.25\r\n",
        "\r\n",
        "act = 'relu'\r\n",
        "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\r\n",
        "\r\n",
        "STAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\r\n",
        "        rate_drop_dense)\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0s-t8q6zMAX"
      },
      "source": [
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\r\n",
        "        binary=True)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4x8i8YWxqTC",
        "outputId": "4242666e-2708-4db7-fb8d-7816e90f055e"
      },
      "source": [
        "# The function \"text_to_wordlist\" is from\r\n",
        "# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\r\n",
        "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\r\n",
        "    # Clean the text, with the option to remove stopwords and to stem words.\r\n",
        "    \r\n",
        "    # Convert words to lower case and split them\r\n",
        "    text = text.lower().split()\r\n",
        "\r\n",
        "    # Optionally, remove stop words\r\n",
        "    if remove_stopwords:\r\n",
        "        stops = set(stopwords.words(\"english\"))\r\n",
        "        text = [w for w in text if not w in stops]\r\n",
        "    \r\n",
        "    text = \" \".join(text)\r\n",
        "\r\n",
        "    # Clean the text\r\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\r\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\r\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\r\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\r\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\r\n",
        "    text = re.sub(r\"n't\", \" not \", text)\r\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\r\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\r\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\r\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\r\n",
        "    text = re.sub(r\",\", \" \", text)\r\n",
        "    text = re.sub(r\"\\.\", \" \", text)\r\n",
        "    text = re.sub(r\"!\", \" ! \", text)\r\n",
        "    text = re.sub(r\"\\/\", \" \", text)\r\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\r\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\r\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\r\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\r\n",
        "    text = re.sub(r\"'\", \" \", text)\r\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\r\n",
        "    text = re.sub(r\":\", \" : \", text)\r\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\r\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\r\n",
        "    text = re.sub(r\" u s \", \" american \", text)\r\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\r\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\r\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\r\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\r\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\r\n",
        "    \r\n",
        "    # Optionally, shorten words to their stems\r\n",
        "    if stem_words:\r\n",
        "        text = text.split()\r\n",
        "        stemmer = SnowballStemmer('english')\r\n",
        "        stemmed_words = [stemmer.stem(word) for word in text]\r\n",
        "        text = \" \".join(stemmed_words)\r\n",
        "    \r\n",
        "    # Return a list of words\r\n",
        "    return(text)\r\n",
        "\r\n",
        "data = pd.read_csv(TRAIN_DATA_FILE)\r\n",
        "texts_1, texts_2, labels = load_data(data)\r\n",
        "print('Found %s texts in train.csv' % len(texts_1))\r\n",
        "\r\n",
        "\r\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\r\n",
        "tokenizer.fit_on_texts(texts_1 + texts_2)\r\n",
        "\r\n",
        "sequences_1 = tokenizer.texts_to_sequences(texts_1)\r\n",
        "sequences_2 = tokenizer.texts_to_sequences(texts_2)\r\n",
        "\r\n",
        "word_index = tokenizer.word_index\r\n",
        "print('Found %s unique tokens' % len(word_index))\r\n",
        "\r\n",
        "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\r\n",
        "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\r\n",
        "labels = np.array(labels)\r\n",
        "print('Shape of data tensor:', data_1.shape)\r\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 404290 texts in train.csv\n",
            "Found 96494 unique tokens\n",
            "Shape of data tensor: (404290, 30)\n",
            "Shape of label tensor: (404290,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw0I1Wugz0qI",
        "outputId": "cc19caf7-32c0-49c4-fc0c-c137174ab304"
      },
      "source": [
        "# prepare embeddings\r\n",
        "\r\n",
        "print('Preparing embedding matrix')\r\n",
        "\r\n",
        "nb_words = min(MAX_NB_WORDS, len(word_index))+1\r\n",
        "\r\n",
        "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\r\n",
        "for word, i in word_index.items():\r\n",
        "    if word in word2vec.vocab:\r\n",
        "        embedding_matrix[i] = word2vec.word_vec(word)\r\n",
        "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\r\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing embedding matrix\n",
            "Null word embeddings: 48484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-54g3WoRz4jl"
      },
      "source": [
        "#sample train/validation data\r\n",
        "\r\n",
        "#np.random.seed(1234)\r\n",
        "perm = np.random.permutation(len(data_1))\r\n",
        "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\r\n",
        "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\r\n",
        "\r\n",
        "data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\r\n",
        "data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\r\n",
        "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\r\n",
        "\r\n",
        "data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\r\n",
        "data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\r\n",
        "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\r\n",
        "\r\n",
        "weight_val = np.ones(len(labels_val))\r\n",
        "if re_weight:\r\n",
        "    weight_val *= 0.472001959\r\n",
        "    weight_val[labels_val==0] = 1.309028344\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVGaewSX0ANM",
        "outputId": "ab8c4126-3e2f-4f12-e113-520e8a889f7c"
      },
      "source": [
        "# define the model structure\r\n",
        "embedding_layer = Embedding(nb_words,\r\n",
        "        EMBEDDING_DIM,\r\n",
        "        weights=[embedding_matrix],\r\n",
        "        input_length=MAX_SEQUENCE_LENGTH,\r\n",
        "        trainable=False)\r\n",
        "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\r\n",
        "\r\n",
        "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\r\n",
        "embedded_sequences_1 = embedding_layer(sequence_1_input)\r\n",
        "x1 = lstm_layer(embedded_sequences_1)\r\n",
        "\r\n",
        "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\r\n",
        "embedded_sequences_2 = embedding_layer(sequence_2_input)\r\n",
        "y1 = lstm_layer(embedded_sequences_2)\r\n",
        "\r\n",
        "merged = concatenate([x1, y1])\r\n",
        "merged = Dropout(rate_drop_dense)(merged)\r\n",
        "merged = BatchNormalization()(merged)\r\n",
        "\r\n",
        "merged = Dense(num_dense, activation=act)(merged)\r\n",
        "merged = Dropout(rate_drop_dense)(merged)\r\n",
        "merged = BatchNormalization()(merged)\r\n",
        "\r\n",
        "preds = Dense(1, activation='sigmoid')(merged)\r\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6H0uiGZ0Ghz"
      },
      "source": [
        "if re_weight:\r\n",
        "    class_weight = {0: 1.309028344, 1: 0.472001959}\r\n",
        "else:\r\n",
        "    class_weight = None"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW49Bibv0KYP",
        "outputId": "3427205a-34f7-4256-b938-bbf15d05f541"
      },
      "source": [
        "# train the model\r\n",
        "\r\n",
        "model = Model(inputs=[sequence_1_input, sequence_2_input], \\\r\n",
        "        outputs=preds)\r\n",
        "model.compile(loss='binary_crossentropy',\r\n",
        "        optimizer='nadam',\r\n",
        "        metrics=['acc'])\r\n",
        "#model.summary()\r\n",
        "print(STAMP)\r\n",
        "\r\n",
        "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\r\n",
        "bst_model_path = STAMP + '.h5'\r\n",
        "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\r\n",
        "\r\n",
        "hist = model.fit([data_1_train, data_2_train], labels_train, \\\r\n",
        "        validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\r\n",
        "        epochs=200, batch_size=2048, shuffle=True, \\\r\n",
        "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\r\n",
        "\r\n",
        "model.load_weights(bst_model_path)\r\n",
        "bst_val_score = min(hist.history['val_loss'])\r\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lstm_199_145_0.22_0.31\n",
            "Epoch 1/200\n",
            "356/356 [==============================] - 124s 327ms/step - loss: 0.5364 - acc: 0.6531 - val_loss: 0.3752 - val_acc: 0.6695\n",
            "Epoch 2/200\n",
            "356/356 [==============================] - 108s 304ms/step - loss: 0.3660 - acc: 0.6991 - val_loss: 0.3367 - val_acc: 0.7394\n",
            "Epoch 3/200\n",
            "356/356 [==============================] - 107s 300ms/step - loss: 0.3454 - acc: 0.7177 - val_loss: 0.3296 - val_acc: 0.7473\n",
            "Epoch 4/200\n",
            "356/356 [==============================] - 108s 304ms/step - loss: 0.3331 - acc: 0.7289 - val_loss: 0.3125 - val_acc: 0.7517\n",
            "Epoch 5/200\n",
            "356/356 [==============================] - 108s 304ms/step - loss: 0.3204 - acc: 0.7412 - val_loss: 0.3034 - val_acc: 0.7671\n",
            "Epoch 6/200\n",
            "356/356 [==============================] - 108s 304ms/step - loss: 0.3112 - acc: 0.7496 - val_loss: 0.2989 - val_acc: 0.7648\n",
            "Epoch 7/200\n",
            "356/356 [==============================] - 109s 306ms/step - loss: 0.3035 - acc: 0.7574 - val_loss: 0.2956 - val_acc: 0.7642\n",
            "Epoch 8/200\n",
            "356/356 [==============================] - 108s 304ms/step - loss: 0.2962 - acc: 0.7643 - val_loss: 0.2997 - val_acc: 0.7842\n",
            "Epoch 9/200\n",
            "356/356 [==============================] - 106s 298ms/step - loss: 0.2899 - acc: 0.7705 - val_loss: 0.2872 - val_acc: 0.7836\n",
            "Epoch 10/200\n",
            "356/356 [==============================] - 107s 301ms/step - loss: 0.2850 - acc: 0.7759 - val_loss: 0.2937 - val_acc: 0.7943\n",
            "Epoch 11/200\n",
            "356/356 [==============================] - 109s 305ms/step - loss: 0.2780 - acc: 0.7824 - val_loss: 0.2828 - val_acc: 0.7926\n",
            "Epoch 12/200\n",
            "356/356 [==============================] - 107s 300ms/step - loss: 0.2746 - acc: 0.7854 - val_loss: 0.2781 - val_acc: 0.7850\n",
            "Epoch 13/200\n",
            "356/356 [==============================] - 107s 299ms/step - loss: 0.2698 - acc: 0.7901 - val_loss: 0.2835 - val_acc: 0.8031\n",
            "Epoch 14/200\n",
            "356/356 [==============================] - 107s 301ms/step - loss: 0.2655 - acc: 0.7954 - val_loss: 0.2835 - val_acc: 0.8077\n",
            "Epoch 15/200\n",
            "356/356 [==============================] - 107s 301ms/step - loss: 0.2630 - acc: 0.7982 - val_loss: 0.2743 - val_acc: 0.8004\n",
            "Epoch 16/200\n",
            "356/356 [==============================] - 107s 300ms/step - loss: 0.2590 - acc: 0.8009 - val_loss: 0.2770 - val_acc: 0.8062\n",
            "Epoch 17/200\n",
            "356/356 [==============================] - 106s 298ms/step - loss: 0.2557 - acc: 0.8043 - val_loss: 0.2810 - val_acc: 0.8083\n",
            "Epoch 18/200\n",
            "356/356 [==============================] - 109s 305ms/step - loss: 0.2518 - acc: 0.8074 - val_loss: 0.2740 - val_acc: 0.8106\n",
            "Epoch 19/200\n",
            "356/356 [==============================] - 107s 301ms/step - loss: 0.2495 - acc: 0.8099 - val_loss: 0.2806 - val_acc: 0.8152\n",
            "Epoch 20/200\n",
            "356/356 [==============================] - 108s 303ms/step - loss: 0.2452 - acc: 0.8137 - val_loss: 0.2704 - val_acc: 0.8075\n",
            "Epoch 21/200\n",
            "356/356 [==============================] - 108s 302ms/step - loss: 0.2429 - acc: 0.8168 - val_loss: 0.2744 - val_acc: 0.8126\n",
            "Epoch 22/200\n",
            "356/356 [==============================] - 108s 304ms/step - loss: 0.2406 - acc: 0.8178 - val_loss: 0.2851 - val_acc: 0.8175\n",
            "Epoch 23/200\n",
            "356/356 [==============================] - 109s 306ms/step - loss: 0.2377 - acc: 0.8208 - val_loss: 0.2649 - val_acc: 0.8061\n",
            "Epoch 24/200\n",
            "356/356 [==============================] - 108s 303ms/step - loss: 0.2360 - acc: 0.8225 - val_loss: 0.2687 - val_acc: 0.8133\n",
            "Epoch 25/200\n",
            "356/356 [==============================] - 108s 303ms/step - loss: 0.2342 - acc: 0.8245 - val_loss: 0.2748 - val_acc: 0.8217\n",
            "Epoch 26/200\n",
            "356/356 [==============================] - 109s 306ms/step - loss: 0.2310 - acc: 0.8270 - val_loss: 0.2712 - val_acc: 0.8192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVbeD4hxCz6H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}